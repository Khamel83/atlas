{
  "id": "7ec8fa92a746458ab6e596781de294f9",
  "type": "backlog",
  "source": "instapaper_csv",
  "payload": {
    "row": {
      "URL": "https://www.theverge.com/2017/12/6/16741106/deepmind-ai-chess-alphazero-shogi-go",
      "Title": "DeepMind\u2019s AI became a superhuman chess player in a few hours, just for fun The Verge",
      "Selection": "The end-game for Google\u2019s AI subsidiary DeepMind was never beating people at board games. It\u2019s always been about creating something akin to a combustion engine for intelligence \u2014 a generic thinking machine that can be applied to a broad range of challenges. The company is still a long way off achieving this goal, but new research published by its scientists this week suggests they\u2019re at least headed down the right path. \n\nIn the paper, DeepMind describes how a descendant of the AI program that first conquered the board game Go has taught itself to play a number of other games at a superhuman level. After eight hours of self-play, the program bested the AI that first beat the human world Go champion; and after four hours of training, it beat the current world champion chess-playing program, Stockfish. Then for a victory lap, it trained for just two hours and polished off one of the world\u2019s best shogi-playing programs named Elmo (shogi being a Japanese version of chess that\u2019s played on a bigger board).\n\nFor each game, the AI program taught itself how to play\n\nOne of the key advances here is that the new AI program, named AlphaZero, wasn\u2019t specifically designed to play any of these games. In each case, it was given some basic rules (like how knights move in chess, and so on) but was programmed with no other strategies or tactics. It simply got better by playing itself over and over again at an accelerated pace \u2014 a method of training AI known as \u201creinforcement learning.\u201d \n\nUsing reinforcement learning in this way isn\u2019t new in and of itself. DeepMind\u2019s engineers used the same method to create AlphaGo Zero; the AI program that was unveiled this October. But, as this week\u2019s paper describes, the new AlphaZero is a \u201cmore generic version\u201d of the same software, meaning it can be applied to a broader range of tasks without being primed beforehand. \n\nWhat\u2019s remarkable here is that in less than 24 hours, the same computer program was able to teach itself how to play three complex board games at superhuman levels. That\u2019s a new feat for the world of AI.\n\nThis takes DeepMind just that little bit closer to building the generic thinking machine the company dreams of, but major challenges lie ahead. When DeepMind CEO Demis Hassabis showed off AlphaGo Zero earlier this year, he suggested that a future version of the program could help with a range of scientific problems \u2014 from designing new drugs to discovering new materials. But these problems are qualitatively very different to just playing board games, and a whole lot of work needs to be done to find out how exactly algorithms can tackle them. All we can say for certain now, is that artificial intelligence has definitely moved on from just playing chess. \n",
      "Folder": "Feedly",
      "Timestamp": "1512610181",
      "Tags": "[]"
    },
    "source_path": "data/backlog/source_drops/Instapaper-Export-2025-03-26_20_23_56.csv"
  },
  "origin_manifest_id": "497cd2c549404409861b3a1098bd5186",
  "created_at": "2025-11-28T05:10:23.564106Z"
}
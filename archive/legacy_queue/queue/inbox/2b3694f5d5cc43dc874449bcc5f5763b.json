{
  "id": "2b3694f5d5cc43dc874449bcc5f5763b",
  "type": "backlog",
  "source": "instapaper_csv",
  "payload": {
    "row": {
      "URL": "https://qz.com/907896/how-poker-playing-ai-libratus-is-learning-to-negotiate-better-than-any-human/",
      "Title": "How a poker-playing AI is learning to negotiate better than any human Quartz",
      "Selection": "In 2012, a comic made its way around the internet listing games on a scale of how close they were to being dominated by artificial intelligence. Checkers and tic-tac-toe had already been conquered; chess\u2019s human champion had been dethroned, and IBM\u2019s Watson had taken no prisoners on Jeopardy. The \u201cComputers may never outplay humans\u201d section still had its stalwarts: Calvinball\u2014the game in Bill Waterson\u2019s Calvin and Hobbes where the rules are made up on the fly\u2014and Seven Minutes in Heaven. Just one step up, listed under \u201cComputers still lose to top humans,\u201d were Chinese game Go and American pastime poker.\n\nFast-forward to January 30, 2017, at the Robots vs. Brains poker tournament in Pittsburgh, Pennsylvania. Ph.D candidate Noam Brown is sitting next to a professional poker player closing out his 20th day of losing to Libratus, a poker-playing bot that Brown co-created at Carnegie Mellon University. As the game progresses, Brown is lightheartedly fielding questions on a Twitch livestream, where a viewer reminds him of that 2012 comic. \u201cYeahhh,\u201d Brown laughs, pulling the image up in front of him. \u201cI think all of these need to be shifted up.\u201d\n\nA few minutes later, after almost three weeks and 120,000 hands of poker, Libratus finished trouncing its human opponents\u2014four of the world\u2019s best professional poker players\u2014in No Limit Texas Hold \u2018Em. The margins were not slight. Were the games not played with imaginary money, the pros would have lost a combined $1.7 million dollars.\n\nFrom left, Tuomas Sandholm, Noam Brown, and Dong Kim answer viewer questions over Twitch.\n\nLibratus\u2019 challenge\n\nLibratus\u2019 poker victory arrived a little more than nine months after AlphaGo, an AI out of London-based DeepMind, beat Go champion Lee Sedol in a series of seven matches. AlphaGo\u2019s win was the first spotlight event for AI beating humans in a game of our own design since chess savant Garry Kasparov\u2019s groundbreaking 1997 loss to IBM\u2019s DeepBlue. An AI may still be far from figuring out the arbitrary world of Calvinball, but beating humans at challenges involving changing, uncertain situations is exactly what Libratus is designed for. Thrives on, even.\n\nLibratus has played trillions of simulated poker hands, but the game is hardly its ultimate goal, just as Go isn\u2019t the ultimate application of DeepMind\u2019s software, and IBM\u2019s AI research didn\u2019t end with Kasparov\u2019s loss. The algorithms behind Libratus were applied to the card game, but the AI itself is built to efficiently manage any kind of negotiation, and to dismantle any opponent that stands in its way. Brown\u2019s AI could be used in situations like those faced by CEOs, politicians, and even intelligence agencies. We could begin to rely on mechanical brains to dictate foreign policy, or to tell us the best strategy for hostage negotiation. It just starts with poker.\n\nNoam Brown, right, and Tuomas Sandholm check on Libratus during the match.\n\n\u201cI think of [poker] like a benchmark, much like chipmakers have their benchmarks,\u201d says Tuomas Sandholm, the CMU professor who is overseeing Libratus and co-created it with Brown. \u201cThere\u2019s no ambiguity in the rules, so you could say technique A is better than technique B, or technique A is better in these conditions, or in these ways.\u201d\n\nSandholm has been working on building algorithms that thrive in uncertainty for 27 years. His work in automating negotiations has been applied to the United Network for Organ sharing, optimizing the process of pairing donated kidneys with recipients in two-thirds of US transplant centers and saving thousands of lives in the process. His 1997 startup, CombineNet, applied that work to the auctioning process, and has managed $60 billion in spending.\n\nSandholm believes the idea powering Libratus is even more powerful.\n\nDrilling for the perfect game\n\nWhen thinking about any game, there\u2019s an assumption that a perfect strategy exists. This perfect strategy, called the Nash Equilibrium by game theorists, would mean that no other set of moves could serve the player better. It\u2019s the objective pinnacle of play.\u00a0Beating humans at challenges involving changing, uncertain situations is exactly what Libratus is designed for.\u00a0\n\nLibratus\u2019 goal is to find that perfect strategy. For No Limits Texas Hold\u2019em, you can imagine this strategy as the core of the Earth: We know it\u2019s there. We can reason what it should look like and what it\u2019s made of, but we can\u2019t get there to actually study it. Now think of Libratus as the world\u2019s biggest drill.\n\nThe algorithm doesn\u2019t function like most of the other AI you\u2019ve heard about. It\u2019s not a deep neural network\u2014the AI behind tools like Facebook\u2019s automatic photo-tagging and Google\u2019s Inbox Smart Reply. Before the tournament, Libratus had never seen a human play poker. The AI has three modules, each performing a different task. Module 1 learns the game of poker, trying to find strategies that work for every situation. Module 2 is the decision-maker for each action the bot takes during a hand. And Module 3 is updated consistently, recording and folding the new information from each move into the system.\n\nPoker player Jason Les lost $880,000 to Libratus.\n\nLearning to play\n\nIn order to beat anyone at poker, Libratus had to first learn the game. Much like a human player, it did that by practicing against an opponent to learn which strategies work and which don\u2019t. Unlike a human, it played trillions of games over 15 million simulated hours, all against a clone of itself. This is called reinforcement learning: The algorithm slowly learns from its own experiences, rather than by analyzing external data like games played by humans.\n\nThe first game Libratus played was completely random. Bet, fold, raise, check, call\u2014they all meant the same thing. But after each hand, Libratus analyzes its strategy against the strategy of its opponent. Because the reward in poker is extremely easy to measure\u2014money\u2014the bot learns after each round which action could have made more money, and stores that strategy for the next time the same situation arises. The actions taken that don\u2019t end in winning are \u201cpruned;\u201d they\u2019re not even considered the next time the bot finds itself in that situation, which saves the AI precious processing time.\n\nThat kind of efficiency is crucial: Libratus is playing trillions and trillions of hands, and No Limit Texas Hold\u2019em has\u2026 no limit, meaning an infinite amount of betting can happen.\n\nJason Les, left, and Daniel McAuley played 20 days of poker against the bot.\n\nTo save the supercomputer from needing to calculate for an infinite number of possibilities, Brown and Sandholm made a simpler, slightly less complex version of the game. All the rules were the same, but instead of being able to bet in $1 increments, the bot could only bet in specifically designated amounts like $200, $250, $300, $400, and $500. To the machine, $290 was the same as $300; the system would just round up. Brown calls this technique \u201caction abstraction.\u201d\n\nThey used a similar trick for the cards themselves. Instead of calculating every potential hand, Brown set similar poker hands to be seen as identical to the AI. There\u2019s not much difference between a Queen-high flush and a King-high flush, for example, so it\u2019s rare the bot would need to know the difference. It would just need to know how to act when presented with a royal-high flush. Teaching the AI to think in broad strokes paid off: Brown says that the card abstractions alone reduced the complexity of the game by a factor of 100,000. If either card or action abstraction were twice as large, the AI would require twice as much time and memory to compute.\n\nPoker pros challenged Libratus to 120,000 total hands.\n\nKnowing that, it doesn\u2019t really seem like Libratus is playing poker at all, but rather a shadow of the game; or as Brown calls it, an abstraction. He makes no equivocation that the first module isn\u2019t really playing poker. It might have calculated a Nash Equilibrium, but it\u2019s for a much simpler game.\n\nThe end-game solver\n\nWhile the first module\u2019s job is to be certain of what\u2019s happening on the poker table, the second module deals with the uncertainty of the opponent\u2019s hand. Module 1 might sound like other AI, roughly learning how to do something over trillions of iterations. But Module 2, the ability to work in uncertain situations, is what sets Libratus apart.\n\nPoker involves a lot of uncertainty. In other games, like chess, there are no secrets. The only factors that can influence the game are openly displayed on the board, meaning every variable is known. Even though a queen can be used in millions of strategies, it\u2019s known to exist in a certain place at a certain time. If the queen were invisible or hidden on the board, a chess-playing bot would have to consider millions of alternate realities based on the potential position of the queen, and the resulting strategies that would spawn from each possible position.\n\nIn Texas Hold\u2019em, two of your opponent\u2019s cards are obscured, making 169 potential hands to consider. This is what game theorists call an \u201cimperfect knowledge\u201d game, where you don\u2019t know certain information about your opponent\u2019s position.\n\nTo crack this problem, Libratus\u2019 second module is called the end-game solver\u2014and it\u2019s every bit as devastating as it sounds. While the first module is in charge of strategy on the pre-flop and flop\u2014before and as a new hand is dealt\u2014the end-game solver takes over from the turn to the end of the hand.\n\nThe end-game solver first recalculates the game using no card or action abstraction; it needs to be as precise as possible. It then uses estimates from the first module\u2019s trillions of hands to guess what the opponent probably has. By using the vast amount of data gathered by the first module, the end-game solver runs through estimates of the most probable hands of the opponent, solving for what they should have rather than what they actually do. Every time the opponent makes a move, the end-game solver recalculates, folding the new information into its strategy. This recursive method of computing is able to chop off entire trees of possibilities, based on the probability an opponent would make a certain decision with a certain hand.\n\nDaniel McAuley, right, won back some money on the last day, only to quickly lose it.\n\nBrown says that the end-game solver is what makes Libratus so strong. It\u2019s also what could make it useful in a board room, or a war room. The software looks at its situation, and tries to predict the best move for nearly every potential way the game could play out, based on games it\u2019s played before. When the opponent makes a move, the calculations are made again, with the opposition\u2019s decision informing a narrower set of potential outcomes. In poker, that means maximizing the chance of winning while not knowing the opponent\u2019s cards. In the real world, it could mean knowing when a foreign power is bluffing about military strength.\n\nIt also makes Libratus bold. While artificial intelligence as we know it today has no wants, desires, or human motivations, the construction of an AI is based on its application. That leads Libratus to act a certain way while playing. For instance, it doesn\u2019t care about the competition in which it plays. It won\u2019t be careful to save money, and it doesn\u2019t keep in mind how much it\u2019s up or down on the scoreboard. All it knows is the hand it\u2019s playing, and a desire to win the most money regardless of the risk. If Libratus has a 10% chance of winning $20,000 against a 90% chance of winning nothing or a guaranteed $1,999, it will always take the 10% risk.\n\n\u201cThe bot is fearless,\u201d Brown says. \u201cHumans don\u2019t approach the game that way because even though they might know it is the right thing to do, a part of them is afraid and will hold them back.\u201d\n\n\u00a0The end-game solver is able to chop off entire trees of possibilities, based on the probability an opponent would make a certain decision with a certain hand.\u00a0It\u2019s worth noting that Libratus\u2019 approach only works at scale. The AI was specifically built to play a poker competition of 120,000 hands, during which one big win can outstrip even a dozen tiny losses. On the competition\u2019s final day, for example, Daniel McAuley was down on one table and up on the other. He slowly won small pot after small pot on his losing table; over 20 or so hands, down $10,000 became down $4,000, which turned into a little less than $2,000 above.\n\nThe reversal of fortune was swift. Within seconds, McAuley was down $4,000 again, then down $8,000. Libratus was brutally effective, its impact evident in the nearly imperceptible grimace that flashed across McAuley\u2019s face.\n\nShoring its defenses\n\nLibratus\u2019 first two modules are what make it strong, but Module 3 is what makes it smarter over time. Initiated once the AI began to play against humans, this part of Libratus runs each night, analyzing the three bet sizes it gauged to be most important based on what the humans bet the most. It allows the AI to learn.\n\nCase in point: During the tournament, the poker professionals figured out that betting in odd numbers would mess up the AI, causing it to respond strangely. They repeatedly took advantage of that weak spot \u2014betting $333 on every good hand, for instance\u2014effectively shooting up a flare over the hole in the machine\u2019s knowledge. The next day the flaw was gone. It wasn\u2019t that Libratus was learning anything it couldn\u2019t have from the first module; Module 3 just let it fill in the gaps more quickly.\n\nBy the time the players realized that the machine was onto them and starting coordinating their bets, it was too late. \u201cOver time the holes became so small,\u201d Brown says. \u201cThey were not able to effectively exploit them, and their situation became hopeless.\u201d\n\nDaniel McAuley gets a high-five after finishing his last hand of poker in the tournament.\n",
      "Folder": "Feedly",
      "Timestamp": "1487043307",
      "Tags": "[]"
    },
    "source_path": "data/backlog/source_drops/Instapaper-Export-2025-03-26_20_23_56.csv"
  },
  "origin_manifest_id": "497cd2c549404409861b3a1098bd5186",
  "created_at": "2025-11-28T05:10:23.713375Z"
}
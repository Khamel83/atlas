# PODCAST PROCESSING MASTER PLAN - COMPLETE IMPLEMENTATION GUIDE

## CURRENT IMPLEMENTATION STATUS

This document outlines the original master plan for podcast processing. Significant progress has been made, and some aspects have been implemented differently or superseded by the `SmartTranscriptionPipeline`.

**Key Updates:**
- **RSS Feed Ingestion:** Implemented directly within `helpers/smart_transcription_pipeline.py` using `feedparser`, driven by `config/podcasts_prioritized_cleaned.csv`.
- **Podcast Configuration:** Centralized in `config/podcasts_prioritized_cleaned.csv`, including RSS URLs and processing preferences.
- **Episode Queuing:** Episodes are queued for processing in `processing_queue.db` by `SmartTranscriptionPipeline`.
- **Transcription:** Handled by `SmartTranscriptionPipeline` via Mac Mini integration (SSH + Whisper), or by direct transcript fetching for `transcript_only` podcasts.
- **Database:** Transcripts are saved to `atlas.db`'s `content` table.

**Remaining Critical Step:** Full setup and configuration of the Mac Mini for transcription processing.

## CRITICAL PROBLEM ANALYSIS

**ORIGINAL BROKEN STATE (largely addressed):**
- Database had 6 "podcast" entries that were actually ARTICLES about podcasts (now clarified with dedicated podcast processing).
- No RSS feed ingestion system existed (now integrated into `SmartTranscriptionPipeline`).
- Scheduler ran 3 minutes then exited (addressed by `SmartTranscriptionPipeline`'s continuous processing and queuing).
- All podcast processing code processed nothing because no episodes existed (now episodes are discovered and queued).
- User expects thousands of real podcast episodes with transcripts (now achievable through the implemented pipeline).

**CURRENT FOCUS:** Ensuring the Mac Mini transcription setup is fully operational for end-to-end processing.

**ROOT CAUSE (original):** Built processing systems without data ingestion foundation (now resolved for podcasts).

## COMPLETE ARCHITECTURE DESIGN

```
RSS FEEDS (37 podcasts)
    ↓
RSS MONITOR (continuous polling)
    ↓
EPISODE DATABASE (real episodes with audio URLs)
    ↓
PROCESSING QUEUE (transcription jobs)
    ↓
TRANSCRIPTION SYSTEM (Mac Mini/cloud)
    ↓
ATLAS DATABASE (searchable transcripts)
    ↓
WEB UI (browse actual podcast episodes)
```

## DATABASE SCHEMA ADDITIONS

*(Note: The `SmartTranscriptionPipeline` currently uses the `content` table in `atlas.db` for storing processed transcripts and the `processing_queue` table in `processing_queue.db` for managing transcription jobs. The schema below represents a more granular design that could be implemented in the future for more detailed episode tracking.)*

```sql
-- New table for actual podcast episodes
CREATE TABLE podcast_episodes (
    id INTEGER PRIMARY KEY AUTOINCREMENT,
    podcast_name TEXT NOT NULL,
    title TEXT NOT NULL,
    audio_url TEXT NOT NULL,
    episode_url TEXT UNIQUE NOT NULL,
    pub_date TEXT,
    duration INTEGER,
    file_size INTEGER,
    description TEXT,
    processed BOOLEAN DEFAULT 0,
    transcript TEXT,
    processing_status TEXT DEFAULT 'pending', -- pending, downloading, transcribing, completed, failed
    error_message TEXT,
    created_at TEXT DEFAULT CURRENT_TIMESTAMP,
    updated_at TEXT DEFAULT CURRENT_TIMESTAMP
);

CREATE INDEX idx_podcast_episodes_name ON podcast_episodes(podcast_name);
CREATE INDEX idx_podcast_episodes_processed ON podcast_episodes(processed);
CREATE INDEX idx_podcast_episodes_status ON podcast_episodes(processing_status);

-- RSS feed tracking
CREATE TABLE podcast_feeds (
    id INTEGER PRIMARY KEY AUTOINCREMENT,
    name TEXT UNIQUE NOT NULL,
    rss_url TEXT NOT NULL,
    category TEXT,
    priority INTEGER DEFAULT 5,
    max_episodes INTEGER DEFAULT 100,
    transcript_only BOOLEAN DEFAULT 0,
    active BOOLEAN DEFAULT 1,
    last_checked TEXT,
    last_episode_date TEXT,
    error_count INTEGER DEFAULT 0,
    created_at TEXT DEFAULT CURRENT_TIMESTAMP
);
```

## STEP 1: RSS FEED INGESTION SYSTEM

*(Note: This functionality is now integrated directly into `helpers/smart_transcription_pipeline.py`, which reads from `config/podcasts_prioritized_cleaned.csv` for RSS URLs and episode preferences.)*

**File: `podcast_rss_monitor.py`**

```python
#!/usr/bin/env python3
"""
Real RSS Feed Monitor for Podcast Episodes
Polls RSS feeds continuously and stores actual episodes
"""

import feedparser
import sqlite3
import time
import requests
import logging
from datetime import datetime, timedelta
from typing import List, Dict, Optional
import os
from pathlib import Path

logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

class PodcastRSSMonitor:
    def __init__(self, db_path: str = "data/atlas.db"):
        self.db_path = db_path
        self.ensure_tables()

    def ensure_tables(self):
        """Create podcast tables if they don't exist"""
        with sqlite3.connect(self.db_path) as conn:
            conn.executescript("""
            CREATE TABLE IF NOT EXISTS podcast_episodes (
                id INTEGER PRIMARY KEY AUTOINCREMENT,
                podcast_name TEXT NOT NULL,
                title TEXT NOT NULL,
                audio_url TEXT NOT NULL,
                episode_url TEXT UNIQUE NOT NULL,
                pub_date TEXT,
                duration INTEGER,
                file_size INTEGER,
                description TEXT,
                processed BOOLEAN DEFAULT 0,
                transcript TEXT,
                processing_status TEXT DEFAULT 'pending',
                error_message TEXT,
                created_at TEXT DEFAULT CURRENT_TIMESTAMP,
                updated_at TEXT DEFAULT CURRENT_TIMESTAMP
            );

            CREATE INDEX IF NOT EXISTS idx_podcast_episodes_name ON podcast_episodes(podcast_name);
            CREATE INDEX IF NOT EXISTS idx_podcast_episodes_processed ON podcast_episodes(processed);
            CREATE INDEX IF NOT EXISTS idx_podcast_episodes_status ON podcast_episodes(processing_status);

            CREATE TABLE IF NOT EXISTS podcast_feeds (
                id INTEGER PRIMARY KEY AUTOINCREMENT,
                name TEXT UNIQUE NOT NULL,
                rss_url TEXT NOT NULL,
                category TEXT,
                priority INTEGER DEFAULT 5,
                max_episodes INTEGER DEFAULT 100,
                transcript_only BOOLEAN DEFAULT 0,
                active BOOLEAN DEFAULT 1,
                last_checked TEXT,
                last_episode_date TEXT,
                error_count INTEGER DEFAULT 0,
                created_at TEXT DEFAULT CURRENT_TIMESTAMP
            );
            """)

    def add_feed(self, name: str, rss_url: str, category: str = "General",
                 max_episodes: int = 100, transcript_only: bool = False):
        """Add a podcast feed to monitor"""
        with sqlite3.connect(self.db_path) as conn:
            conn.execute("""
                INSERT OR REPLACE INTO podcast_feeds
                (name, rss_url, category, max_episodes, transcript_only)
                VALUES (?, ?, ?, ?, ?)
            """, (name, rss_url, category, max_episodes, transcript_only))
        logger.info(f"Added feed: {name}")

    def load_prioritized_feeds(self):
        """Load feeds from prioritized CSV and add to database"""
        import csv
        csv_path = "config/podcasts_prioritized.csv"

        if not os.path.exists(csv_path):
            logger.error(f"Prioritized CSV not found: {csv_path}")
            return

        # RSS URLs for prioritized podcasts (YOU NEED TO POPULATE THESE)
        rss_urls = {
            "Acquired": "https://feeds.simplecast.com/7wT59F0l",
            "99% Invisible": "https://feeds.99percentinvisible.org/99percentinvisible",
            "This American Life": "https://feeds.thisamericanlife.org/talpodcast",
            "Radiolab": "https://feeds.feedburner.com/radiolab",
            "Accidental Tech Podcast": "https://atp.fm/rss",
            "ACQ2 by Acquired": "https://feeds.simplecast.com/YbhYRFqN",
            "Stratechery": "https://stratechery.fm/feed/",
            "The Vergecast": "https://feeds.megaphone.fm/vergecast",
            "Hard Fork": "https://feeds.simplecast.com/7y1CbAbN",
            "Planet Money": "https://www.npr.org/rss/podcast.php?id=510289",
            "The Indicator from Planet Money": "https://www.npr.org/rss/podcast.php?id=510325",
            # ADD MORE RSS URLS FOR ALL 37 PODCASTS
        }

        with open(csv_path, 'r') as f:
            reader = csv.DictReader(f)
            for row in reader:
                name = row['Podcast Name']
                if name in rss_urls:
                    self.add_feed(
                        name=name,
                        rss_url=rss_urls[name],
                        category=row['Category'],
                        max_episodes=int(row['Count']),
                        transcript_only=bool(int(row['Transcript_Only']))
                    )
                else:
                    logger.warning(f"No RSS URL for podcast: {name}")

    def check_feed(self, feed_name: str, rss_url: str, max_episodes: int = 100):
        """Check single RSS feed and store new episodes"""
        try:
            logger.info(f"Checking feed: {feed_name}")
            feed = feedparser.parse(rss_url)

            if not feed.entries:
                logger.warning(f"No entries found for {feed_name}")
                return 0

            new_episodes = 0
            for entry in feed.entries[:max_episodes]:
                episode_url = entry.get('link', '')
                if not episode_url:
                    continue

                # Find audio URL
                audio_url = None
                if hasattr(entry, 'enclosures'):
                    for enclosure in entry.enclosures:
                        if enclosure.type.startswith('audio/'):
                            audio_url = enclosure.href
                            break

                if not audio_url:
                    continue

                # Check if episode already exists
                with sqlite3.connect(self.db_path) as conn:
                    exists = conn.execute(
                        "SELECT id FROM podcast_episodes WHERE episode_url = ?",
                        (episode_url,)
                    ).fetchone()

                    if not exists:
                        conn.execute("""
                            INSERT INTO podcast_episodes
                            (podcast_name, title, audio_url, episode_url, pub_date, description)
                            VALUES (?, ?, ?, ?, ?, ?)
                        """, (
                            feed_name,
                            entry.get('title', 'Unknown'),
                            audio_url,
                            episode_url,
                            entry.get('published', ''),
                            entry.get('summary', '')
                        ))
                        new_episodes += 1

            # Update feed status
            with sqlite3.connect(self.db_path) as conn:
                conn.execute("""
                    UPDATE podcast_feeds
                    SET last_checked = CURRENT_TIMESTAMP, error_count = 0
                    WHERE name = ?
                """, (feed_name,))

            logger.info(f"Added {new_episodes} new episodes from {feed_name}")
            return new_episodes

        except Exception as e:
            logger.error(f"Error checking feed {feed_name}: {e}")
            with sqlite3.connect(self.db_path) as conn:
                conn.execute("""
                    UPDATE podcast_feeds
                    SET error_count = error_count + 1
                    WHERE name = ?
                """, (feed_name,))
            return 0

    def check_all_feeds(self):
        """Check all active feeds for new episodes"""
        with sqlite3.connect(self.db_path) as conn:
            feeds = conn.execute("""
                SELECT name, rss_url, max_episodes
                FROM podcast_feeds
                WHERE active = 1
            """).fetchall()

        total_new = 0
        for name, rss_url, max_episodes in feeds:
            total_new += self.check_feed(name, rss_url, max_episodes)

        logger.info(f"Total new episodes found: {total_new}")
        return total_new

    def run_continuous(self, interval_minutes: int = 30):
        """Run continuous monitoring"""
        logger.info(f"Starting continuous RSS monitoring (every {interval_minutes} minutes)")

        while True:
            try:
                self.check_all_feeds()
                logger.info(f"Sleeping for {interval_minutes} minutes...")
                time.sleep(interval_minutes * 60)
            except KeyboardInterrupt:
                logger.info("Stopping RSS monitor")
                break
            except Exception as e:
                logger.error(f"Error in RSS monitoring: {e}")
                time.sleep(60)  # Wait 1 minute on error

if __name__ == "__main__":
    import argparse
    parser = argparse.ArgumentParser()
    parser.add_argument('--load-feeds', action='store_true', help='Load feeds from CSV')
    parser.add_argument('--check-once', action='store_true', help='Check feeds once and exit')
    parser.add_argument('--continuous', action='store_true', help='Run continuously')
    parser.add_argument('--interval', type=int, default=30, help='Check interval in minutes')

    args = parser.parse_args()

    monitor = PodcastRSSMonitor()

    if args.load_feeds:
        monitor.load_prioritized_feeds()
    elif args.check_once:
        monitor.check_all_feeds()
    elif args.continuous:
        monitor.run_continuous(args.interval)
    else:
        parser.print_help()
```

## STEP 2: FIX CONTINUOUS SCHEDULER

*(Note: The `SmartTranscriptionPipeline` manages its own processing queue and scheduling for transcription jobs. This scheduler could be used for other continuous tasks.)*

**File: `scripts/atlas_scheduler_fixed.py`**

```python
#!/usr/bin/env python3
"""
Fixed Atlas Scheduler - Runs Continuously
No more 3-minute exits, proper continuous operation
"""

import sys
import time
import signal
import logging
import sqlite3
from pathlib import Path

sys.path.insert(0, str(Path(__file__).parent.parent))

logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

class ContinuousScheduler:
    def __init__(self):
        self.running = True
        self.db_path = "data/atlas.db"

        # Signal handlers
        signal.signal(signal.SIGINT, self.stop)
        signal.signal(signal.SIGTERM, self.stop)

    def stop(self, signum=None, frame=None):
        logger.info("Stopping scheduler...")
        self.running = False

    def check_for_work(self):
        """Check if there's actual work to do"""
        try:
            with sqlite3.connect(self.db_path) as conn:
                # Check for unprocessed podcast episodes
                unprocessed = conn.execute("""
                    SELECT COUNT(*) FROM podcast_episodes
                    WHERE processing_status = 'pending'
                """).fetchone()[0]

                if unprocessed > 0:
                    logger.info(f"Found {unprocessed} episodes to process")
                    self.process_episodes(conn, limit=5)
                    return True

        except Exception as e:
            logger.error(f"Error checking for work: {e}")

        return False

    def process_episodes(self, conn, limit=5):
        """Process pending episodes"""
        episodes = conn.execute("""
            SELECT id, podcast_name, title, audio_url
            FROM podcast_episodes
            WHERE processing_status = 'pending'
            ORDER BY created_at ASC
            LIMIT ?
        """, (limit,)).fetchall()

        for episode_id, podcast_name, title, audio_url in episodes:
            logger.info(f"Processing: {title[:50]}...")

            # Mark as downloading
            conn.execute("""
                UPDATE podcast_episodes
                SET processing_status = 'downloading', updated_at = CURRENT_TIMESTAMP
                WHERE id = ?
            """, (episode_id,))
            conn.commit()

            # HERE: Add actual download and transcription logic
            # For now, mark as completed after brief pause
            time.sleep(2)

            conn.execute("""
                UPDATE podcast_episodes
                SET processing_status = 'completed',
                    processed = 1,
                    transcript = 'PLACEHOLDER TRANSCRIPT',
                    updated_at = CURRENT_TIMESTAMP
                WHERE id = ?
            """, (episode_id,))
            conn.commit()

    def run(self):
        logger.info("Starting continuous scheduler...")
        logger.info("Will check for work every 30 seconds")

        while self.running:
            try:
                work_found = self.check_for_work()
                if not work_found:
                    logger.info("No work found, sleeping...")

                # Sleep for 30 seconds
                for _ in range(30):
                    if not self.running:
                        break
                    time.sleep(1)

            except Exception as e:
                logger.error(f"Scheduler error: {e}")
                time.sleep(10)

        logger.info("Scheduler stopped")

if __name__ == "__main__":
    scheduler = ContinuousScheduler()
    scheduler.run()
```

## STEP 3: EPISODE PROCESSING PIPELINE

*(Note: This functionality is now handled by the `SmartTranscriptionPipeline`'s internal logic, which queues episodes for audio download and transcription, often leveraging Mac Mini integration.)*

**File: `podcast_episode_processor.py`**

```python
#!/usr/bin/env python3
"""
Complete Episode Processing Pipeline
Downloads audio, transcribes, stores in Atlas
"""

import os
import subprocess
import requests
import sqlite3
import tempfile
import logging
from pathlib import Path

logger = logging.getLogger(__name__)

class EpisodeProcessor:
    def __init__(self, db_path: str = "data/atlas.db"):
        self.db_path = db_path
        self.temp_dir = Path("/tmp/podcast_processing")
        self.temp_dir.mkdir(exist_ok=True)

    def download_audio(self, audio_url: str, output_path: Path) -> bool:
        """Download audio file"""
        try:
            logger.info(f"Downloading: {audio_url}")
            response = requests.get(audio_url, stream=True, timeout=300)
            response.raise_for_status()

            with open(output_path, 'wb') as f:
                for chunk in response.iter_content(chunk_size=8192):
                    f.write(chunk)

            logger.info(f"Downloaded to: {output_path}")
            return True

        except Exception as e:
            logger.error(f"Download failed: {e}")
            return False

    def transcribe_audio(self, audio_path: Path) -> str:
        """Transcribe audio using whisper"""
        try:
            # Check if whisper is available
            result = subprocess.run(
                ["whisper", str(audio_path), "--model", "base", "--output_format", "txt"],
                capture_output=True,
                text=True,
                timeout=1800  # 30 minutes
            )

            if result.returncode == 0:
                # Find transcript file
                transcript_path = audio_path.with_suffix('.txt')
                if transcript_path.exists():
                    return transcript_path.read_text()

            logger.error(f"Transcription failed: {result.stderr}")
            return ""

        except Exception as e:
            logger.error(f"Transcription error: {e}")
            return ""

    def process_episode(self, episode_id: int) -> bool:
        """Process single episode"""
        with sqlite3.connect(self.db_path) as conn:
            episode = conn.execute("""
                SELECT podcast_name, title, audio_url
                FROM podcast_episodes WHERE id = ?
            """, (episode_id,)).fetchone()

            if not episode:
                return False

            podcast_name, title, audio_url = episode

            # Create temp file for audio
            temp_file = self.temp_dir / f"episode_{episode_id}.mp3"

            try:
                # Download audio
                conn.execute("""
                    UPDATE podcast_episodes
                    SET processing_status = 'downloading'
                    WHERE id = ?
                """, (episode_id,))
                conn.commit()

                if not self.download_audio(audio_url, temp_file):
                    raise Exception("Download failed")

                # Transcribe
                conn.execute("""
                    UPDATE podcast_episodes
                    SET processing_status = 'transcribing'
                    WHERE id = ?
                """, (episode_id,))
                conn.commit()

                transcript = self.transcribe_audio(temp_file)

                if not transcript:
                    raise Exception("Transcription failed")

                # Store transcript
                conn.execute("""
                    UPDATE podcast_episodes
                    SET processing_status = 'completed',
                        processed = 1,
                        transcript = ?
                    WHERE id = ?
                """, (transcript, episode_id))

                # Add to main Atlas content table for searching
                conn.execute("""
                    INSERT OR REPLACE INTO content
                    (title, content, content_type, url, metadata, created_at)
                    VALUES (?, ?, 'podcast_episode', ?, ?, CURRENT_TIMESTAMP)
                """, (
                    f"[PODCAST] {title}",
                    transcript,
                    audio_url,
                    f'{{"podcast_name": "{podcast_name}", "episode_id": {episode_id}}}'
                ))

                conn.commit()

                logger.info(f"Successfully processed: {title}")
                return True

            except Exception as e:
                logger.error(f"Processing failed for episode {episode_id}: {e}")
                conn.execute("""
                    UPDATE podcast_episodes
                    SET processing_status = 'failed',
                        error_message = ?
                    WHERE id = ?
                """, (str(e), episode_id))
                conn.commit()
                return False

            finally:
                # Cleanup temp file
                if temp_file.exists():
                    temp_file.unlink()
```

## STEP 4: INTEGRATION AND STARTUP SCRIPTS

*(Note: This startup script is now largely superseded by running `smart_transcription.py` directly with `--process-all` or `--process-queue` arguments.)*

**File: `start_podcast_system.sh`**

```bash
#!/bin/bash
# Complete Podcast System Startup Script

echo "🎙️ Starting Complete Podcast Processing System"

# Activate virtual environment
source venv/bin/activate

# Step 1: Load RSS feeds
echo "📡 Loading RSS feeds..."
python podcast_rss_monitor.py --load-feeds

# Step 2: Check feeds once to populate initial episodes
echo "🔍 Initial feed check..."
python podcast_rss_monitor.py --check-once

# Step 3: Start continuous RSS monitoring in background
echo "🔄 Starting RSS monitor..."
nohup python podcast_rss_monitor.py --continuous --interval 30 > logs/rss_monitor.log 2>&1 &
RSS_PID=$!
echo "RSS Monitor PID: $RSS_PID"

# Step 4: Start continuous scheduler
echo "⚡ Starting scheduler..."
nohup python scripts/atlas_scheduler_fixed.py > logs/scheduler.log 2>&1 &
SCHEDULER_PID=$!
echo "Scheduler PID: $SCHEDULER_PID"

# Step 5: Start web API if not running
echo "🌐 Checking web API..."
if ! curl -s http://localhost:7444/api/v1/search/?query=test&limit=1 > /dev/null; then
    echo "Starting Atlas API..."
    nohup python atlas_service_manager.py start > logs/api.log 2>&1 &
fi

echo "✅ All systems started!"
echo "📊 Monitor with:"
echo "   tail -f logs/rss_monitor.log"
echo "   tail -f logs/scheduler.log"
echo "   curl 'http://localhost:7444/api/v1/search/?query=podcast&limit=5'"

echo "$RSS_PID" > pids/rss_monitor.pid
echo "$SCHEDULER_PID" > pids/scheduler.pid

echo "🎉 Podcast system is now running continuously!"
```

## STEP 5: RSS URLS FOR ALL 37 PODCASTS

*(Note: This is now managed in `config/podcasts_prioritized_cleaned.csv`, which includes the RSS URLs for each podcast.)*

**You need to populate this in `podcast_rss_monitor.py`:**

```python
# COMPLETE RSS URL MAPPING - ADD ALL 37 FEEDS
rss_urls = {
    # High Priority Shows
    "Acquired": "https://feeds.simplecast.com/7wT59F0l",
    "ACQ2 by Acquired": "https://feeds.simplecast.com/YbhYRFqN",
    "99% Invisible": "https://feeds.99percentinvisible.org/99percentinvisible",
    "This American Life": "https://feeds.thisamericanlife.org/talpodcast",
    "Radiolab": "https://feeds.feedburner.com/radiolab",
    "Accidental Tech Podcast": "https://atp.fm/rss",

    # Tech & Business
    "Stratechery": "https://stratechery.fm/feed/",
    "The Vergecast": "https://feeds.megaphone.fm/vergecast",
    "Hard Fork": "https://feeds.simplecast.com/7y1CbAbN",
    "Decoder with Nilay Patel": "https://feeds.megaphone.fm/decoder",
    "Sharp Tech with Ben Thompson": "https://stratechery.fm/feed/",
    "The Cognitive Revolution": "[FIND RSS URL]",

    # Economics & Finance
    "Planet Money": "https://www.npr.org/rss/podcast.php?id=510289",
    "The Indicator from Planet Money": "https://www.npr.org/rss/podcast.php?id=510325",
    "Slate Money": "[FIND RSS URL]",

    # Politics & News
    "Political Gabfest": "https://feeds.megaphone.fm/gabfest",
    "The NPR Politics Podcast": "https://www.npr.org/rss/podcast.php?id=510310",
    "Today, Explained": "https://feeds.megaphone.fm/explained",

    # Long-form & Education
    "Conversations with Tyler": "[FIND RSS URL]",
    "EconTalk": "https://feeds.feedburner.com/EcontalkPodcast",
    "Lex Fridman Podcast": "https://lexfridman.com/feed/podcast/",
    "The Ezra Klein Show": "https://feeds.simplecast.com/82FI35Px",
    "The Knowledge Project with Shane Parrish": "[FIND RSS URL]",

    # Culture & Entertainment
    "The Rewatchables": "https://feeds.megaphone.fm/the-rewatchables",
    "The Prestige TV Podcast": "[FIND RSS URL]",
    "Slate Culture": "[FIND RSS URL]",

    # Food
    "The Recipe with Kenji and Deb": "[FIND RSS URL]",
    "Recipe Club": "[FIND RSS URL]",
    "Ringer Food": "[FIND RSS URL]",

    # ADD REMAINING FEEDS - Search for "[Podcast Name] RSS feed" to find URLs
}
```

## EXECUTION CHECKLIST (Updated)

**Current Status:** The core podcast ingestion and queuing system is implemented and functional. The primary remaining task is to configure the Mac Mini for transcription.

1.  **Ensure `podcasts_prioritized_cleaned.csv` is up-to-date:**
    *   Verify all desired podcasts and their RSS URLs are correctly listed in `config/podcasts_prioritized_cleaned.csv`.

2.  **Configure Mac Mini for Transcription:**
    *   Ensure `config/mac_mini.json` is correctly configured with the Mac Mini's hostname/IP, SSH user, and SSH key path.
    *   Verify SSH access from this machine to the Mac Mini.
    *   Ensure `whisper` is installed and accessible on the Mac Mini.

3.  **Run Podcast Processing:**
    *   To process all prioritized podcasts and queue episodes for transcription:
        ```bash
        python3 smart_transcription.py --process-all
        ```
    *   To process the transcription queue (requires Mac Mini setup):
        ```bash
        python3 smart_transcription.py --process-queue
        ```

4.  **Verify Working:**
    *   Check `data/atlas.db` for new entries in the `content` table.
    *   Check `data/processing_queue.db` for the status of queued jobs.
    *   Monitor logs in `data/podcasts/smart_transcription.log` for processing details.

**SUCCESS METRICS:**
- `atlas.db` contains searchable podcast transcripts.
- `processing_queue.db` shows completed transcription jobs.
- System runs continuously (when `smart_transcription.py --process-all` is scheduled).

This updated checklist reflects the current state of the project. Once the Mac Mini is configured, the system will be fully operational for end-to-end podcast processing.
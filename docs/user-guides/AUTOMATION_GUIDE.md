# Atlas Automation and Workflows Guide\n\nThis guide helps you set up automated content workflows in Atlas, from RSS feed automation to custom integration with popular automation tools. Learn how to save time and ensure your content is always up-to-date with minimal manual effort.\n\n## Table of Contents\n\n1. [RSS Feed Automation](#rss-feed-automation)\n2. [Email Forwarding](#email-forwarding)\n3. [Scheduled Processing](#scheduled-processing)\n4. [Integration Examples](#integration-examples)\n5. [Custom Scripts](#custom-scripts)\n6. [Monitoring and Alerts](#monitoring-and-alerts)\n\n## RSS Feed Automation\n\n### Auto-Ingesting Blog Posts and News\n\nAtlas can automatically discover, download, and process new content from RSS feeds:\n\n#### Setting Up RSS Feed Automation\n\n1. **Create an OPML file** with your subscriptions:\n   ```xml\n   <?xml version=\"1.0\" encoding=\"UTF-8\"?>\n   <opml version=\"2.0\">\n     <head>\n       <title>My Atlas Feeds</title>\n     </head>\n     <body>\n       <outline text=\"Technology\">\n         <outline \n           title=\"Tech News\" \n           type=\"rss\" \n           xmlUrl=\"https://example.com/tech-rss\" />\n         <outline \n           title=\"Programming Blog\" \n           type=\"rss\" \n           xmlUrl=\"https://example.com/programming-rss\" />\n       </outline>\n     </body>\n   </opml>\n   ```\n\n2. **Save the OPML file** as `inputs/podcasts.opml` (the same location used for podcasts)\n\n3. **Configure the automation**:\n   ```bash\n   # Add to your crontab for daily processing\n   0 9 * * * cd /home/ubuntu/dev/atlas && python run.py --podcasts\n   ```\n\n#### Advanced RSS Configuration\n\n##### Content Filtering\nFilter content by keywords or categories:\n```python\n# In your configuration\nRSS_FILTERS = {\n    \"tech_news\": {\n        \"include_keywords\": [\"AI\", \"machine learning\", \"programming\"],\n        \"exclude_keywords\": [\"advertising\", \"sponsored\"],\n        \"min_length\": 500,  # Minimum characters\n        \"categories\": [\"technology\", \"science\"]\n    }\n}\n```\n\n##### Rate Limiting\nPrevent overwhelming sources with rate limiting:\n```python\n# In your configuration\nRSS_RATE_LIMITS = {\n    \"example.com\": {\n        \"requests_per_hour\": 10,\n        \"delay_between_requests\": 60  # seconds\n    }\n}\n```\n\n#### Troubleshooting RSS Automation\n\nCommon issues and solutions:\n\n1. **\"Feed not found\" errors**:\n   - Verify RSS URLs are correct\n   - Check if feeds require authentication\n   - Test feeds in a browser\n\n2. **\"No new content\"**:\n   - Check feed update frequency\n   - Verify filtering settings\n   - Review last processed timestamps\n\n3. **\"Rate limit exceeded\"**:\n   - Implement delays between requests\n   - Use caching to reduce requests\n   - Contact feed providers for higher limits\n\n## Email Forwarding\n\n### Automatically Process Forwarded Emails\n\nAtlas can automatically process emails sent to a dedicated address:\n\n#### Setting Up Email Automation\n\n1. **Configure email account** in Atlas settings:\n   ```env\n   # In your .env file\n   EMAIL_PROVIDER=gmail\n   EMAIL_USERNAME=your-email@gmail.com\n   EMAIL_PASSWORD=your-app-password\n   EMAIL_POLL_INTERVAL=300  # Check every 5 minutes\n   ```\n\n2. **Forward emails** to your Atlas email address:\n   - Set up forwarding in your email provider\n   - Use filters to automatically forward specific senders\n   - Create rules to tag emails for Atlas processing\n\n#### Email Processing Features\n\n1. **Content Extraction**:\n   - Extract text from email body\n   - Process attachments (PDFs, documents)\n   - Preserve formatting and structure\n\n2. **Categorization**:\n   - Auto-categorize by sender\n   - Tag based on content keywords\n   - Apply custom rules for organization\n\n3. **Response Handling**:\n   - Send automated acknowledgments\n   - Generate follow-up actions\n   - Integrate with task management\n\n#### Advanced Email Configuration\n\n##### Custom Processing Rules\n```python\n# Custom email processing rules\nEMAIL_RULES = [\n    {\n        \"name\": \"Newsletter Processing\",\n        \"conditions\": {\n            \"from\": \"*@newsletter.com\",\n            \"subject_contains\": [\"weekly\", \"update\"]\n        },\n        \"actions\": {\n            \"category\": \"newsletters\",\n            \"tags\": [\"weekly\", \"updates\"],\n            \"process_attachments\": True\n        }\n    },\n    {\n        \"name\": \"Receipt Processing\",\n        \"conditions\": {\n            \"from\": \"*@receipts.com\",\n            \"has_attachments\": True\n        },\n        \"actions\": {\n            \"category\": \"finance\",\n            \"extract_data\": [\"amount\", \"date\", \"merchant\"],\n            \"send_to_finance_app\": True\n        }\n    }\n]\n```\n\n##### Security Settings\n```python\n# Email security configuration\nEMAIL_SECURITY = {\n    \"allowed_senders\": [\n        \"trusted@example.com\",\n        \"newsletter@company.com\"\n    ],\n    \"blocked_senders\": [\n        \"*@spam.com\",\n        \"noreply@*\"\n    ],\n    \"max_attachment_size\": 25 * 1024 * 1024,  # 25MB\n    \"allowed_attachment_types\": [\n        \"pdf\", \"docx\", \"txt\", \"jpg\", \"png\"\n    ]\n}\n```\n\n## Scheduled Processing\n\n### Setting Up Daily/Weekly Content Batches\n\nAtlas can automatically process content on a schedule:\n\n#### Cron-Based Scheduling\n\n1. **Edit your crontab**:\n   ```bash\n   crontab -e\n   ```\n\n2. **Add scheduled jobs**:\n   ```bash\n   # Daily article processing at 8 AM\n   0 8 * * * cd /home/ubuntu/dev/atlas && python run.py --articles\n\n   # Weekly podcast processing on Sundays at 9 AM\n   0 9 * * 0 cd /home/ubuntu/dev/atlas && python run.py --podcasts\n\n   # Daily YouTube processing at 10 AM\n   0 10 * * * cd /home/ubuntu/dev/atlas && python run.py --youtube\n\n   # Hourly transcript discovery\n   0 * * * * cd /home/ubuntu/dev/atlas && python run.py --transcripts\n   ```\n\n#### Systemd Timer Scheduling\n\nFor more advanced scheduling, use systemd timers:\n\n1. **Create a service file** (`/etc/systemd/system/atlas-daily.service`):\n   ```ini\n   [Unit]\n   Description=Atlas Daily Processing\n   After=network-online.target\n\n   [Service]\n   Type=oneshot\n   User=ubuntu\n   WorkingDirectory=/home/ubuntu/dev/atlas\n   ExecStart=/home/ubuntu/dev/atlas/venv/bin/python run.py --all\n   ```\n\n2. **Create a timer file** (`/etc/systemd/system/atlas-daily.timer`):\n   ```ini\n   [Unit]\n   Description=Run Atlas Daily Processing\n   Requires=atlas-daily.service\n\n   [Timer]\n   OnCalendar=daily\n   Persistent=true\n\n   [Install]\n   WantedBy=timers.target\n   ```\n\n3. **Enable and start the timer**:\n   ```bash\n   sudo systemctl daemon-reload\n   sudo systemctl enable atlas-daily.timer\n   sudo systemctl start atlas-daily.timer\n   ```\n\n#### Custom Scheduling Scripts\n\nCreate custom scripts for complex scheduling logic:\n\n```python\n#!/usr/bin/env python3\n# custom_scheduler.py\n\nimport os\nimport sys\nfrom datetime import datetime, timedelta\nfrom pathlib import Path\n\n# Add Atlas to path\nsys.path.insert(0, str(Path(__file__).parent))\n\nfrom helpers.config import load_config\nfrom run import main as run_atlas\n\ndef should_run_daily_processing():\n    \"\"\"Determine if daily processing should run\"\"\"\n    # Custom logic here\n    return True\n\ndef should_run_weekly_processing():\n    \"\"\"Determine if weekly processing should run\"\"\"\n    today = datetime.now()\n    # Run on Sundays\n    return today.weekday() == 6\n\ndef main():\n    config = load_config()\n    \n    # Daily processing\n    if should_run_daily_processing():\n        print(\"Running daily processing...\")\n        # Simulate command line arguments\n        sys.argv = ['run.py', '--all']\n        run_atlas()\n    \n    # Weekly processing\n    if should_run_weekly_processing():\n        print(\"Running weekly processing...\")\n        # Custom weekly tasks\n        run_weekly_tasks(config)\n\ndef run_weekly_tasks(config):\n    \"\"\"Run weekly maintenance tasks\"\"\"\n    # Add your custom weekly tasks here\n    print(\"Weekly tasks completed\")\n\nif __name__ == \"__main__\":\n    main()\n```\n\nSchedule the custom script:\n```bash\n# Run every day at 7 AM\n0 7 * * * cd /home/ubuntu/dev/atlas && python custom_scheduler.py\n```\n\n## Integration Examples\n\n### IFTTT, Zapier, and Other Automation Tools\n\nAtlas can integrate with popular automation platforms:\n\n#### IFTTT Integration\n\n1. **Create an IFTTT applet**:\n   - Trigger: \"New feed item\" from RSS\n   - Action: \"Make a web request\" to Atlas API\n   - URL: `http://your-atlas-server:8000/api/v1/content/save`\n   - Method: POST\n   - Content Type: `application/json`\n   - Body:\n     ```json\n     {\n       \"title\": \"{{EntryTitle}}\",\n       \"url\": \"{{EntryUrl}}\",\n       \"content\": \"{{EntryContent}}\"\n     }\n     ```\n\n2. **Configure Atlas API endpoint**:\n   ```python\n   # In your web API\n   @app.post(\"/api/v1/content/save\")\n   async def save_content(content_data: ContentData):\n       # Process the content\n       result = process_content(content_data)\n       return {\"status\": \"success\", \"id\": result.id}\n   ```\n\n#### Zapier Integration\n\n1. **Create a Zap**:\n   - Trigger: \"New Email\" from Gmail\n   - Action: \"Webhooks by Zapier\" - Custom Request\n   - URL: `http://your-atlas-server:8000/api/v1/email/process`\n   - Method: POST\n   - Data:\n     ```json\n     {\n       \"subject\": \"{{Subject}}\",\n       \"sender\": \"{{FromName}}\",\n       \"content\": \"{{BodyPlain}}\",\n       \"attachments\": \"{{Attachments}}\"\n     }\n     ```\n\n2. **Handle the webhook**:\n   ```python\n   # In your web API\n   @app.post(\"/api/v1/email/process\")\n   async def process_email(email_data: EmailData):\n       # Process the email\n       result = handle_email(email_data)\n       return {\"status\": \"processed\", \"items_created\": len(result.items)}\n   ```\n\n#### Custom Webhooks\n\nCreate custom webhook endpoints for any automation tool:\n\n```python\n# webhook_handlers.py\nfrom fastapi import APIRouter, HTTPException\nfrom pydantic import BaseModel\nfrom typing import List, Optional\nimport json\n\nrouter = APIRouter()\n\nclass WebhookData(BaseModel):\n    \"\"\"Generic webhook data model\"\"\"\n    source: str\n    event_type: str\n    payload: dict\n    timestamp: Optional[str] = None\n\n@router.post(\"/webhooks/generic\")\nasync def handle_generic_webhook(data: WebhookData):\n    \"\"\"Handle generic webhook events\"\"\"\n    try:\n        # Route based on source and event type\n        if data.source == \"github\" and data.event_type == \"push\":\n            return handle_github_push(data.payload)\n        elif data.source == \"twitter\" and data.event_type == \"tweet\":\n            return handle_twitter_tweet(data.payload)\n        else:\n            # Default processing\n            return process_generic_event(data)\n    except Exception as e:\n        raise HTTPException(status_code=500, detail=str(e))\n\ndef handle_github_push(payload: dict):\n    \"\"\"Handle GitHub push events\"\"\"\n    # Extract commit information\n    commits = payload.get(\"commits\", [])\n    for commit in commits:\n        # Process each commit\n        process_commit(commit)\n    \n    return {\"status\": \"processed\", \"commits\": len(commits)}\n\ndef handle_twitter_tweet(payload: dict):\n    \"\"\"Handle Twitter tweet events\"\"\"\n    # Extract tweet information\n    tweet_text = payload.get(\"text\", \"\")\n    user = payload.get(\"user\", {}).get(\"screen_name\", \"\")\n    \n    # Process the tweet\n    result = process_tweet(tweet_text, user)\n    \n    return {\"status\": \"processed\", \"tweet_id\": result.id}\n\ndef process_generic_event(data: WebhookData):\n    \"\"\"Process generic webhook events\"\"\"\n    # Generic processing logic\n    result = store_webhook_event(data)\n    return {\"status\": \"stored\", \"event_id\": result.id}\n```\n\n## Custom Scripts\n\n### Simple Examples for Power Users\n\nCreate custom scripts to extend Atlas automation:\n\n#### Content Aggregator Script\n\n```python\n#!/usr/bin/env python3\n# content_aggregator.py\n\nimport requests\nimport json\nfrom datetime import datetime, timedelta\nfrom typing import List, Dict\nimport os\n\nclass ContentAggregator:\n    \"\"\"Aggregate content from multiple sources\"\"\"\n    \n    def __init__(self, atlas_api_url: str):\n        self.atlas_api_url = atlas_api_url\n        self.session = requests.Session()\n        \n    def aggregate_reddit_posts(self, subreddit: str, limit: int = 10) -> List[Dict]:\n        \"\"\"Aggregate top posts from a subreddit\"\"\"\n        url = f\"https://www.reddit.com/r/{subreddit}/top.json\"\n        params = {\"limit\": limit, \"t\": \"day\"}\n        \n        try:\n            response = self.session.get(url, params=params)\n            response.raise_for_status()\n            data = response.json()\n            \n            posts = []\n            for post in data[\"data\"][\"children\"]:\n                post_data = post[\"data\"]\n                posts.append({\n                    \"title\": post_data[\"title\"],\n                    \"url\": f\"https://reddit.com{post_data['permalink']}\",\n                    \"content\": post_data[\"selftext\"],\n                    \"source\": f\"Reddit /r/{subreddit}\",\n                    \"timestamp\": datetime.fromtimestamp(post_data[\"created_utc\"]).isoformat()\n                })\n            \n            return posts\n        except Exception as e:\n            print(f\"Error aggregating Reddit posts: {e}\")\n            return []\n    \n    def aggregate_hacker_news(self, limit: int = 10) -> List[Dict]:\n        \"\"\"Aggregate top stories from Hacker News\"\"\"\n        try:\n            # Get top stories\n            response = self.session.get(\"https://hacker-news.firebaseio.com/v0/topstories.json\")\n            response.raise_for_status()\n            story_ids = response.json()[:limit]\n            \n            posts = []\n            for story_id in story_ids:\n                story_response = self.session.get(\n                    f\"https://hacker-news.firebaseio.com/v0/item/{story_id}.json\"\n                )\n                story_response.raise_for_status()\n                story = story_response.json()\n                \n                posts.append({\n                    \"title\": story[\"title\"],\n                    \"url\": story.get(\"url\", f\"https://news.ycombinator.com/item?id={story_id}\"),\n                    \"content\": story.get(\"text\", \"\"),\n                    \"source\": \"Hacker News\",\n                    \"timestamp\": datetime.fromtimestamp(story[\"time\"]).isoformat()\n                })\n            \n            return posts\n        except Exception as e:\n            print(f\"Error aggregating Hacker News: {e}\")\n            return []\n    \n    def save_to_atlas(self, content_items: List[Dict]) -> Dict:\n        \"\"\"Save aggregated content to Atlas\"\"\"\n        saved_count = 0\n        errors = []\n        \n        for item in content_items:\n            try:\n                response = self.session.post(\n                    f\"{self.atlas_api_url}/api/v1/content/save\",\n                    json=item\n                )\n                response.raise_for_status()\n                saved_count += 1\n            except Exception as e:\n                errors.append(f\"Failed to save {item['title']}: {e}\")\n        \n        return {\n            \"saved_count\": saved_count,\n            \"total_items\": len(content_items),\n            \"errors\": errors\n        }\n\ndef main():\n    \"\"\"Main aggregation function\"\"\"\n    atlas_api_url = os.getenv(\"ATLAS_API_URL\", \"https://atlas.khamel.com\")\n    aggregator = ContentAggregator(atlas_api_url)\n    \n    # Aggregate content\n    print(\"Aggregating Reddit posts...\")\n    reddit_posts = aggregator.aggregate_reddit_posts(\"technology\", 5)\n    \n    print(\"Aggregating Hacker News...\")\n    hn_posts = aggregator.aggregate_hacker_news(5)\n    \n    # Combine and save\n    all_posts = reddit_posts + hn_posts\n    print(f\"Saving {len(all_posts)} posts to Atlas...\")\n    \n    result = aggregator.save_to_atlas(all_posts)\n    print(f\"Saved {result['saved_count']} of {result['total_items']} posts\")\n    \n    if result['errors']:\n        print(\"Errors:\")\n        for error in result['errors']:\n            print(f\"  - {error}\")\n\nif __name__ == \"__main__\":\n    main()\n```\n\n#### Daily Summary Script\n\n```python\n#!/usr/bin/env python3\n# daily_summary.py\n\nimport requests\nimport json\nfrom datetime import datetime, timedelta\nfrom typing import List, Dict\nimport os\n\nclass DailySummaryGenerator:\n    \"\"\"Generate daily content summaries\"\"\"\n    \n    def __init__(self, atlas_api_url: str):\n        self.atlas_api_url = atlas_api_url\n        self.session = requests.Session()\n        \n    def get_todays_content(self) -> List[Dict]:\n        \"\"\"Get content processed today\"\"\"\n        try:\n            # Calculate date range\n            today = datetime.now().date()\n            start_of_day = datetime.combine(today, datetime.min.time())\n            \n            # Query Atlas API for today's content\n            response = self.session.get(\n                f\"{self.atlas_api_url}/api/v1/content\",\n                params={\n                    \"start_date\": start_of_day.isoformat(),\n                    \"limit\": 100\n                }\n            )\n            response.raise_for_status()\n            return response.json().get(\"items\", [])\n        except Exception as e:\n            print(f\"Error fetching today's content: {e}\")\n            return []\n    \n    def generate_summary(self, content_items: List[Dict]) -> str:\n        \"\"\"Generate a text summary of content\"\"\"\n        if not content_items:\n            return \"No content processed today.\"\n        \n        # Group by category\n        categories = {}\n        for item in content_items:\n            category = item.get(\"category\", \"uncategorized\")\n            if category not in categories:\n                categories[category] = []\n            categories[category].append(item)\n        \n        # Generate summary\n        summary_lines = [\n            f\"Daily Content Summary - {datetime.now().strftime('%Y-%m-%d')}\"],\n            f\"Total items processed: {len(content_items)}\",\n            \"\"\n        ]\n        \n        for category, items in categories.items():\n            summary_lines.append(f\"{category.title()} ({len(items)} items):\")\n            for item in items[:5]:  # Show top 5 per category\n                title = item.get(\"title\", \"Untitled\")\n                summary_lines.append(f\"  - {title}\")\n            if len(items) > 5:\n                summary_lines.append(f\"  ... and {len(items) - 5} more\")\n            summary_lines.append(\"\")\n        \n        return \"\\n\".join(summary_lines)\n    \n    def save_summary(self, summary_text: str) -> bool:\n        \"\"\"Save summary as a note in Atlas\"\"\"\n        try:\n            response = self.session.post(\n                f\"{self.atlas_api_url}/api/v1/notes\",\n                json={\n                    \"title\": f\"Daily Summary - {datetime.now().strftime('%Y-%m-%d')}\"],\n                    \"content\": summary_text,\n                    \"category\": \"summaries\",\n                    \"tags\": [\"daily\", \"summary\"]\n                }\n            )\n            response.raise_for_status()\n            return True\n        except Exception as e:\n            print(f\"Error saving summary: {e}\")\n            return False\n\ndef main():\n    \"\"\"Generate and save daily summary\"\"\"\n    atlas_api_url = os.getenv(\"ATLAS_API_URL\", \"https://atlas.khamel.com\")\n    generator = DailySummaryGenerator(atlas_api_url)\n    \n    print(\"Fetching today's content...\")\n    content_items = generator.get_todays_content()\n    \n    print(f\"Generating summary for {len(content_items)} items...\")\n    summary = generator.generate_summary(content_items)\n    \n    print(\"Summary:\")\n    print(summary)\n    print()\n    \n    print(\"Saving summary to Atlas...\")\n    if generator.save_summary(summary):\n        print(\"Summary saved successfully!\")\n    else:\n        print(\"Failed to save summary.\")\n\nif __name__ == \"__main__\":\n    main()\n```\n\n## Monitoring and Alerts\n\n### Setup Monitoring for Automation Failures\n\nEnsure your automated workflows run smoothly with proper monitoring:\n\n#### Health Check Script\n\n```python\n#!/usr/bin/env python3\n# automation_monitor.py\n\nimport requests\nimport json\nimport smtplib\nfrom email.mime.text import MIMEText\nfrom email.mime.multipart import MIMEMultipart\nfrom datetime import datetime, timedelta\nimport os\nimport sys\n\nclass AutomationMonitor:\n    \"\"\"Monitor Atlas automation health\"\"\"\n    \n    def __init__(self, atlas_api_url: str):\n        self.atlas_api_url = atlas_api_url\n        self.session = requests.Session()\n        \n    def check_atlas_status(self) -> Dict:\n        \"\"\"Check if Atlas is running and healthy\"\"\"\n        try:\n            response = self.session.get(f\"{self.atlas_api_url}/api/v1/health\")\n            response.raise_for_status()\n            return response.json()\n        except Exception as e:\n            return {\"status\": \"error\", \"error\": str(e)}\n    \n    def check_recent_processing(self) -> Dict:\n        \"\"\"Check if content has been processed recently\"\"\"\n        try:\n            # Check last 24 hours\n            since = datetime.now() - timedelta(hours=24)\n            \n            response = self.session.get(\n                f\"{self.atlas_api_url}/api/v1/content/stats\",\n                params={\"since\": since.isoformat()}\n            )\n            response.raise_for_status()\n            stats = response.json()\n            \n            # Check if any content was processed\n            total_processed = sum(stats.get(\"by_type\", {}).values())\n            \n            return {\n                \"status\": \"ok\" if total_processed > 0 else \"warning\",\n                \"processed_count\": total_processed,\n                \"period\": \"24h\"\n            }\n        except Exception as e:\n            return {\"status\": \"error\", \"error\": str(e)}\n    \n    def check_disk_space(self) -> Dict:\n        \"\"\"Check available disk space\"\"\"\n        try:\n            import shutil\n            total, used, free = shutil.disk_usage(\"/\")\n            \n            # Convert to GB\n            free_gb = free / (1024**3)\n            total_gb = total / (1024**3)\n            \n            status = \"ok\" if free_gb > 5 else \"critical\"  # Alert if less than 5GB\n            \n            return {\n                \"status\": status,\n                \"free_gb\": round(free_gb, 2),\n                \"total_gb\": round(total_gb, 2),\n                \"usage_percent\": round((used / total) * 100, 2)\n            }\n        except Exception as e:\n            return {\"status\": \"error\", \"error\": str(e)}\n    \n    def check_log_errors(self) -> Dict:\n        \"\"\"Check for recent errors in logs\"\"\"\n        try:\n            # Check last hour of logs\n            since = datetime.now() - timedelta(hours=1)\n            \n            response = self.session.get(\n                f\"{self.atlas_api_url}/api/v1/logs/errors\",\n                params={\"since\": since.isoformat()}\n            )\n            response.raise_for_status()\n            errors = response.json().get(\"errors\", [])\n            \n            return {\n                \"status\": \"ok\" if len(errors) == 0 else \"warning\",\n                \"error_count\": len(errors),\n                \"recent_errors\": errors[:5]  # Show first 5 errors\n            }\n        except Exception as e:\n            return {\"status\": \"error\", \"error\": str(e)}\n    \n    def generate_report(self) -> Dict:\n        \"\"\"Generate comprehensive health report\"\"\"\n        report = {\n            \"timestamp\": datetime.now().isoformat(),\n            \"checks\": {\n                \"atlas_status\": self.check_atlas_status(),\n                \"recent_processing\": self.check_recent_processing(),\n                \"disk_space\": self.check_disk_space(),\n                \"log_errors\": self.check_log_errors()\n            }\n        }\n        \n        # Overall status\n        statuses = [check[\"status\"] for check in report[\"checks\"].values()]\n        if \"critical\" in statuses:\n            report[\"overall_status\"] = \"critical\"\n        elif \"error\" in statuses:\n            report[\"overall_status\"] = \"error\"\n        elif \"warning\" in statuses:\n            report[\"overall_status\"] = \"warning\"\n        else:\n            report[\"overall_status\"] = \"ok\"\n        \n        return report\n    \n    def send_alert(self, report: Dict, recipients: List[str]):\n        \"\"\"Send alert email if issues detected\"\"\"\n        if report[\"overall_status\"] == \"ok\":\n            return  # No alert needed\n        \n        # Email configuration\n        smtp_server = os.getenv(\"SMTP_SERVER\", \"localhost\")\n        smtp_port = int(os.getenv(\"SMTP_PORT\", \"587\"))\n        smtp_user = os.getenv(\"SMTP_USER\")\n        smtp_password = os.getenv(\"SMTP_PASSWORD\")\n        from_email = os.getenv(\"FROM_EMAIL\", \"atlas@localhost\")\n        \n        if not smtp_user or not smtp_password:\n            print(\"SMTP credentials not configured, skipping alert\")\n            return\n        \n        # Create message\n        msg = MIMEMultipart()\n        msg[\"From\"] = from_email\n        msg[\"To\"] = \", \".join(recipients)\n        msg[\"Subject\"] = f\"Atlas Automation Alert - {report['overall_status'].upper()}\"\n        \n        # Create body\n        body = f\"\"\"\nAtlas Automation System Alert\n\nOverall Status: {report['overall_status'].upper()}\n\nDetailed Checks:\n\"\"\"\n        \n        for check_name, check_result in report[\"checks\"].items():\n            body += f\"\\n{check_name}: {check_result['status'].upper()}\"\n            if \"error\" in check_result:\n                body += f\" - {check_result['error']}\"\n            if \"processed_count\" in check_result:\n                body += f\" - {check_result['processed_count']} items processed\"\n            if \"free_gb\" in check_result:\n                body += f\" - {check_result['free_gb']} GB free\"\n            if \"error_count\" in check_result:\n                body += f\" - {check_result['error_count']} recent errors\"\n        \n        body += f\"\"\"\n\nReport generated at: {report['timestamp']}\n        \nPlease check the Atlas system and logs for more details.\n        \"\"\"\n        \n        msg.attach(MIMEText(body, \"plain\"))\n        \n        # Send email\n        try:\n            server = smtplib.SMTP(smtp_server, smtp_port)\n            server.starttls()\n            server.login(smtp_user, smtp_password)\n            server.send_message(msg)\n            server.quit()\n            print(\"Alert email sent successfully\")\n        except Exception as e:\n            print(f\"Failed to send alert email: {e}\")\n\ndef main():\n    \"\"\"Run automation monitoring\"\"\"\n    atlas_api_url = os.getenv(\"ATLAS_API_URL\", \"https://atlas.khamel.com\")\n    monitor = AutomationMonitor(atlas_api_url)\n    \n    print(\"Running automation health checks...\")\n    report = monitor.generate_report()\n    \n    print(f\"Overall status: {report['overall_status']}\"))\n    for check_name, check_result in report[\"checks\"].items():\n        print(f\"  {check_name}: {check_result['status']}\"))\n    \n    # Send alert if needed\n    alert_recipients = os.getenv(\"ALERT_RECIPIENTS\")\n    if alert_recipients and report[\"overall_status\"] != \"ok\":\n        recipients = [email.strip() for email in alert_recipients.split(\",\")]\n        monitor.send_alert(report, recipients)\n    \n    # Exit with error code if critical issues\n    if report[\"overall_status\"] in [\"critical\", \"error\"]:\n        sys.exit(1)\n\nif __name__ == \"__main__\":\n    main()\n```\n\n#### Setting Up Monitoring\n\n1. **Schedule the monitor script**:\n   ```bash\n   # Check every hour\n   0 * * * * cd /home/ubuntu/dev/atlas && python automation_monitor.py\n   ```\n\n2. **Configure alerting** in your `.env` file:\n   ```env\n   SMTP_SERVER=smtp.gmail.com\n   SMTP_PORT=587\n   SMTP_USER=your-email@gmail.com\n   SMTP_PASSWORD=your-app-password\n   ALERT_RECIPIENTS=admin@yourcompany.com,ops@yourcompany.com\n   ```\n\n3. **Create a dashboard** to visualize monitoring data:\n   ```python\n   # monitoring_dashboard.py\n   from fastapi import FastAPI, Request\n   from fastapi.templating import Jinja2Templates\n   import json\n   from datetime import datetime\n   \n   app = FastAPI()\n   templates = Jinja2Templates(directory=\"templates\")\n   \n   @app.get(\"/monitoring\")\n   async def monitoring_dashboard(request: Request):\n       # Load recent reports\n       reports = load_recent_reports()\n       \n       return templates.TemplateResponse(\"monitoring.html\", {\n           \"request\": request,\n           \"reports\": reports,\n           \"current_status\": get_current_status()\n       })\n   \n   def load_recent_reports():\n       \"\"\"Load recent monitoring reports\"\"\"\n       # Implementation depends on how you store reports\n       pass\n   \n   def get_current_status():\n       \"\"\"Get current system status\"\"\"\n       # Implementation depends on your monitoring setup\n       pass\n   ```\n\nThis guide provides a comprehensive overview of automation and workflow capabilities in Atlas. With these tools and examples, you can create powerful automated systems that keep your content processing running smoothly with minimal manual intervention.\n\nHappy automating! 🤖✨
## ğŸ“š Instapaper Ingestion Module â€“ Full Integration Design

> **Note**: This document was provided by the project owner on *2025-07-11* and copied here verbatim so the full design lives inside the repository.  It has not yet been fully implemented â€“ see the TODO section at the end for next steps.

---

### ğŸ§­ Overview

This module is responsible for fetching, storing, and processing all articles saved to an Instapaper account. It supports both **initial full export** and **incremental updates**, using either credentials or OAuth 1.0a. All output is stored locally in a structured format, ready for downstream processing by the Atlas ingest pipeline.

---

### ğŸ¯ Goals

- Pull all saved bookmarks from Instapaper using their public API.
- Store both raw and normalized versions of the data locally.
- Deduplicate entries using unique Instapaper `hash` values or URLs.
- Output clean files (CSV or JSONL) for use in Atlas ingestion.
- Allow manual future syncing by re-running the script with diff logic.
- Preserve folder/tag metadata for future categorization or training.

---

### ğŸ” Required .env Values

```env
# For basic auth (recommended for one-time pull)
INSTAPAPER_USERNAME=you@example.com
INSTAPAPER_PASSWORD=yourpassword

# Or for OAuth 1.0a (more secure if syncing long-term)
INSTAPAPER_CONSUMER_KEY=your_key
INSTAPAPER_CONSUMER_SECRET=your_secret
INSTAPAPER_ACCESS_TOKEN=token
INSTAPAPER_ACCESS_SECRET=secret

# Always required
INSTAPAPER_API_BASE=https://www.instapaper.com/api/1
```

---

### ğŸ§± API Capabilities
* `/bookmarks/list` â€“ fetch all bookmarks
* `/bookmarks/list?have=` â€“ skip previously-seen items via hash
* `/folders/list` â€“ (optional) folder metadata
* `/bookmarks/get_text` â€“ (optional) clean article text

Returned fields per item: `title`, `url`, `description`, `hash`, `time`, `progress`, `starred`, `folder_id` â€¦

---

### ğŸ“ Directory Structure
```
input/
â””â”€â”€ instapaper/
    â”œâ”€â”€ raw/
    â”‚   â””â”€â”€ instapaper_full_2025-07-11.json
    â”œâ”€â”€ clean/
    â”‚   â””â”€â”€ clean_instapaper_2025-07-11.csv
    â”œâ”€â”€ hashes/
    â”‚   â””â”€â”€ instapaper_hashes.txt
    â””â”€â”€ archive/
        â””â”€â”€ (optional backup of older files)
```

---

### ğŸ§© Ingest Logic (Step-by-Step)
1. **Authenticate** â€“ basic auth or OAuth.
2. **Fetch All Bookmarks** â€“ `GET /bookmarks/list` + pagination; store raw JSON.
3. **Deduplicate** â€“ compare `hash` against local `hashes/instapaper_hashes.txt`.
4. **Normalize** â€“ extract `title`, `url`, `folder_title`, `created_at`, `hash` â€¦
5. **Write Output** â€“ `raw/`, `clean/`, `hashes/`.
6. **Feed to Atlas** â€“ pass each normalized link into central ingest dispatcher.

---

### ğŸ” Optional Syncing Strategy
* On future runs use `?have=` with saved hashes to fetch only new items.
* Hash list stored in flat file is sufficient; no DB required.

---

### ğŸ¤– Automation Tips
* Schedule via cron or run manually.
* Future upgrade: Playwright auto-login + export.

---

### ğŸ“Œ Questions / Decisions Outstanding
* Parse full article via `bookmarks/get_text` now or defer to Atlas?
* Map folders to tags or keep as metadata?
* Include timestamps / device info?
* How to dedupe across RSS + Instapaper duplicates?
* Unify all link ingestion via a `unified_link_ingestor.py`?

---

### âœ… Done When
* All bookmarks fetched & stored.
* Normalized files saved under `input/instapaper/`.
* Atlas ingestion ready (or triggered).
* Future syncs skip duplicates via hash file.

---

### TODO (tracked via GitHub issues / TODO list)
- [ ] Implement `instapaper_api_export.py` following this spec
- [ ] Add incremental sync flag & hash storage
- [ ] Decide on folderâ†’tag mapping strategy
- [ ] Wire export output into main ingest dispatcher
#!/usr/bin/env python3
"""
Atlas Complete Processor - Ensures 100% completion
Processes EVERY item in atlas_queue and creates markdown files
"""

import os
import sqlite3
import requests
from bs4 import BeautifulSoup
import time
import hashlib
from datetime import datetime

def process_all_items():
    """Process every single item in atlas_queue"""
    print("üöÄ ATLAS COMPLETE PROCESSOR - 100% COMPLETION MODE")
    print("=" * 60)

    conn = sqlite3.connect('podcast_processing.db')

    # Get ALL unprocessed items
    cursor = conn.execute("""
        SELECT source_url, title, content, source_type, id
        FROM atlas_queue
        WHERE source_url NOT IN (
            SELECT DISTINCT source_url FROM (
                SELECT SUBSTR(title, 10) as source_url FROM atlas_queue
                WHERE title LIKE 'content_%'
            )
        )
        ORDER BY id
    """)

    items = cursor.fetchall()
    print(f"üìä Found {len(items)} items to process for markdown conversion")

    processed = 0
    errors = 0

    for i, (url, title, content, source_type, item_id) in enumerate(items):
        try:
            print(f"\nüîÑ [{i+1}/{len(items)}] Processing: {url[:80]}...")

            # Process URL and create markdown
            markdown_content = process_url_to_markdown(url, title, source_type)

            if markdown_content:
                # Create filename based on URL hash
                url_hash = hashlib.md5(url.encode()).hexdigest()[:16]
                filename = f"content_{url_hash}.md"
                filepath = f"content/markdown/{filename}"

                # Write markdown file
                with open(filepath, 'w', encoding='utf-8') as f:
                    f.write(markdown_content)

                print(f"‚úÖ Created: {filename}")
                processed += 1
            else:
                print(f"‚ö†Ô∏è  No content extracted from: {url}")
                errors += 1

            # Small delay to be respectful to servers
            time.sleep(1)

        except Exception as e:
            print(f"‚ùå Error processing {url}: {e}")
            errors += 1
            continue

    conn.close()

    print(f"\nüéØ PROCESSING COMPLETE")
    print(f"‚úÖ Processed: {processed}")
    print(f"‚ùå Errors: {errors}")
    print(f"üìÅ Check content/markdown/ for results")

def process_url_to_markdown(url, title, source_type):
    """Convert URL to markdown content"""
    try:
        headers = {
            'User-Agent': 'Mozilla/5.0 (compatible; Atlas-Content-Processor/1.0)'
        }

        response = requests.get(url, headers=headers, timeout=30)
        response.raise_for_status()

        soup = BeautifulSoup(response.content, 'html.parser')

        # Remove script/style elements
        for script in soup(["script", "style"]):
            script.decompose()

        # Extract title
        if soup.title:
            page_title = soup.title.get_text().strip()
        else:
            page_title = title or url

        # Extract main content
        content_text = ""

        # Try common content selectors
        content_selectors = [
            'article', 'main', '[role="main"]',
            '.content', '.post-content', '.entry-content',
            '.article-body', '.story-body'
        ]

        for selector in content_selectors:
            content_element = soup.select_one(selector)
            if content_element:
                content_text = content_element.get_text(separator='\n\n').strip()
                break

        # Fallback to body content
        if not content_text and soup.body:
            content_text = soup.body.get_text(separator='\n\n').strip()

        # Clean up content
        lines = [line.strip() for line in content_text.split('\n') if line.strip()]
        content_text = '\n\n'.join(lines[:100])  # Limit to reasonable length

        # Create markdown
        markdown = f"""# {page_title}

**Source:** {url}
**Type:** {source_type}
**Processed:** {datetime.now().isoformat()}

---

{content_text}

---

*Generated by Atlas Content Processor*
"""

        return markdown

    except Exception as e:
        print(f"  ‚ö†Ô∏è  Error fetching {url}: {e}")
        return None

if __name__ == "__main__":
    process_all_items()
#!/usr/bin/env python3
"""
Test Podcast Deduplication Fix

This test proves that the TranscriptManager fixes the podcast duplication issue
by properly tracking processed URLs and preventing re-processing.
"""

import pytest
import tempfile
import json
from pathlib import Path
from helpers.transcript_manager import TranscriptManager, TranscriptInfo

class TestPodcastDeduplication:
    """Test that podcast duplication is prevented"""

    def setup_method(self):
        """Setup test with temporary directories"""
        self.temp_dir = tempfile.mkdtemp()
        self.config = {
            'output_dir': self.temp_dir
        }
        self.manager = TranscriptManager(self.config)

    def test_deduplication_prevents_reprocessing(self):
        """Test that URLs are not processed twice"""
        test_url = "https://atp.fm/episodes/554"

        # First check - should not be processed
        assert not self.manager.already_processed(test_url)

        # Mark as processed
        self.manager.mark_processed(test_url)

        # Second check - should be processed
        assert self.manager.already_processed(test_url)

        # Verify state persists
        stats = self.manager.get_processing_stats()
        assert stats['total_processed'] >= 1

    def test_multiple_urls_tracked(self):
        """Test that multiple URLs are tracked separately"""
        urls = [
            "https://atp.fm/episodes/554",
            "https://atp.fm/episodes/553",
            "https://atp.fm/episodes/552"
        ]

        # None should be processed initially
        for url in urls:
            assert not self.manager.already_processed(url)

        # Process first URL
        self.manager.mark_processed(urls[0])

        # Only first should be marked processed
        assert self.manager.already_processed(urls[0])
        assert not self.manager.already_processed(urls[1])
        assert not self.manager.already_processed(urls[2])

        # Process second URL
        self.manager.mark_processed(urls[1])

        # First two should be processed, third not
        assert self.manager.already_processed(urls[0])
        assert self.manager.already_processed(urls[1])
        assert not self.manager.already_processed(urls[2])

    def test_state_persistence(self):
        """Test that processed state persists across manager instances"""
        test_url = "https://atp.fm/episodes/555"

        # First manager instance
        manager1 = TranscriptManager(self.config)
        assert not manager1.already_processed(test_url)
        manager1.mark_processed(test_url)
        assert manager1.already_processed(test_url)

        # Second manager instance should load same state
        manager2 = TranscriptManager(self.config)
        assert manager2.already_processed(test_url)

    def test_rss_processing_simulation(self):
        """Simulate the RSS processing that was causing duplicates"""
        # Simulate ATP RSS feed with repeated episodes
        atp_episodes = [
            {"url": "https://atp.fm/episodes/554", "title": "Episode 554: Latest"},
            {"url": "https://atp.fm/episodes/553", "title": "Episode 553: Previous"},
            {"url": "https://atp.fm/episodes/552", "title": "Episode 552: Older"},
            # Simulate RSS feed being processed again with same episodes
            {"url": "https://atp.fm/episodes/554", "title": "Episode 554: Latest"},  # Duplicate!
            {"url": "https://atp.fm/episodes/553", "title": "Episode 553: Previous"}, # Duplicate!
        ]

        processed_count = 0
        skipped_count = 0

        for episode in atp_episodes:
            url = episode["url"]

            if not self.manager.already_processed(url):
                # Would process this episode
                self.manager.mark_processed(url)
                processed_count += 1
                print(f"Processing: {episode['title']}")
            else:
                # Skip duplicate
                skipped_count += 1
                print(f"Skipping duplicate: {episode['title']}")

        # Should process 3 unique episodes, skip 2 duplicates
        assert processed_count == 3
        assert skipped_count == 2

        # Verify final state
        stats = self.manager.get_processing_stats()
        assert stats['total_processed'] >= 3

    def test_podcast_count_stays_stable(self):
        """Test that podcast count doesn't inflate due to duplicates"""
        # Simulate your exact scenario: 272 podcasts that shouldn't become 683

        # Start with baseline count
        baseline_count = 272

        # Simulate processing episodes the first time
        for i in range(baseline_count):
            episode_url = f"https://example.com/episode/{i}"
            self.manager.mark_processed(episode_url)

        initial_stats = self.manager.get_processing_stats()
        initial_count = initial_stats['total_processed']

        # Simulate running the same RSS processing again (what was causing duplicates)
        for i in range(baseline_count):
            episode_url = f"https://example.com/episode/{i}"

            # The key fix: check before processing
            if not self.manager.already_processed(episode_url):
                self.manager.mark_processed(episode_url)
            # Otherwise skip (no duplicate processing)

        final_stats = self.manager.get_processing_stats()
        final_count = final_stats['total_processed']

        # Count should not change (no duplicates processed)
        assert final_count == initial_count
        assert final_count >= baseline_count

        print(f"âœ… Podcast count stable: {initial_count} â†’ {final_count}")

    def test_transcript_info_deduplication(self):
        """Test deduplication at the TranscriptInfo level"""
        # Create transcript info
        transcript1 = TranscriptInfo(
            url="https://atp.fm/episodes/556",
            title="Test Episode",
            source="atp"
        )

        transcript2 = TranscriptInfo(
            url="https://atp.fm/episodes/556",  # Same URL
            title="Test Episode (Duplicate)",
            source="atp"
        )

        # First transcript should be processed
        assert not self.manager.already_processed(transcript1.url)

        # Simulate processing first transcript
        self.manager.mark_processed(transcript1.url)

        # Second transcript (same URL) should be skipped
        assert self.manager.already_processed(transcript2.url)

    def teardown_method(self):
        """Cleanup test data"""
        import shutil
        shutil.rmtree(self.temp_dir, ignore_errors=True)

class TestOldVsNewBehavior:
    """Compare old behavior (duplicates) vs new behavior (no duplicates)"""

    def test_old_behavior_simulation(self):
        """Simulate old behavior that created duplicates"""
        # Old behavior: no deduplication, process everything every time
        episodes = ["ep1", "ep2", "ep3"]

        # First run
        processed_first_run = []
        for ep in episodes:
            processed_first_run.append(ep)  # Would process everything

        # Second run (RSS processed again)
        processed_second_run = []
        for ep in episodes:
            processed_second_run.append(ep)  # Would process everything AGAIN

        total_old_behavior = len(processed_first_run) + len(processed_second_run)
        assert total_old_behavior == 6  # 3 + 3 = 6 (duplicates!)

    def test_new_behavior_with_deduplication(self):
        """Test new behavior with TranscriptManager prevents duplicates"""
        manager = TranscriptManager()
        episodes = ["ep1", "ep2", "ep3"]

        total_processed = 0

        # First run
        for ep in episodes:
            if not manager.already_processed(ep):
                manager.mark_processed(ep)
                total_processed += 1

        # Second run (RSS processed again)
        for ep in episodes:
            if not manager.already_processed(ep):
                manager.mark_processed(ep)
                total_processed += 1
            # else: skip duplicate

        assert total_processed == 3  # Only 3 unique episodes processed

        print("âœ… New behavior prevents duplicates!")

if __name__ == '__main__':
    # Run a quick test
    print("Testing podcast deduplication fix...")

    test = TestPodcastDeduplication()
    test.setup_method()

    try:
        test.test_podcast_count_stays_stable()
        test.test_rss_processing_simulation()
        print("âœ… All deduplication tests passed!")

        # Test old vs new
        old_new_test = TestOldVsNewBehavior()
        old_new_test.test_old_behavior_simulation()
        old_new_test.test_new_behavior_with_deduplication()

    finally:
        test.teardown_method()

    print("ðŸŽ‰ Podcast duplication issue is FIXED!")